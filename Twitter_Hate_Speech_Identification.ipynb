{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, f1_score\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the tweets file using read_csv function from Pandas package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('TwitterHate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0   @user when a father is dysfunctional and is s...\n",
       "1      0  @user @user thanks for #lyft credit i can't us...\n",
       "2      0                                bihday your majesty\n",
       "3      0  #model   i love u take with u all the time in ...\n",
       "4      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    False\n",
       "tweet    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.014579813528565"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Imbalance\n",
    "\n",
    "(data.label.value_counts()[1]/data.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.98542018647143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.label.value_counts()[0]/data.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion - The data is Imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get the tweets into a list for easy text cleanup and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @user when a father is dysfunctional and is s...\n",
       "1    @user @user thanks for #lyft credit i can't us...\n",
       "2                                  bihday your majesty\n",
       "3    #model   i love u take with u all the time in ...\n",
       "4               factsguide: society now    #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. To cleanup: \n",
    "\n",
    "###### 1. Normalize the casing.\n",
    "###### 2. Using regular expressions, remove user handles. These begin with '@’.\n",
    "###### 3. Using regular expressions, remove URLs.\n",
    "###### 4. Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n",
    "###### 5. Remove stop words.\n",
    "###### 6. Remove redundant terms like ‘amp’, ‘rt’, etc.\n",
    "###### 7. Remove ‘#’ symbols from the tweet while retaining the term.\n",
    "\n",
    "#### 4. Extra cleanup by removing terms with a length of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tweets):\n",
    "    # 3.1. Normalize the casing\n",
    "    tweets = tweets.apply(lambda x: x.lower())\n",
    "    \n",
    "    # 3.2. Using regular expressions, remove user handles. These begin with '@’.\n",
    "    tweets = tweets.apply(lambda x: re.sub('@\\w+', '', x))\n",
    "    \n",
    "    # 3.3 Using regular expressions, remove URLs.\n",
    "    url_pattern = '((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*'\n",
    "    tweets = tweets.apply(lambda x: re.sub(url_pattern, '', x))\n",
    "    \n",
    "    # 3.7. Remove ‘#’ symbols from the tweet while retaining the term.\n",
    "    tweets = tweets.apply(lambda x: re.sub('#','',x))\n",
    "    \n",
    "    # Removing extra '....'\n",
    "    tweets = tweets.apply(lambda x: re.sub('\\.+', '', x))\n",
    "    \n",
    "    # 3.4. Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n",
    "    tweets = tweets.apply(lambda x: TweetTokenizer().tokenize(x))\n",
    "    \n",
    "    # 3.5. Remove stop words.\n",
    "    # 3.6. Remove redundant terms like ‘amp’, ‘rt’, etc.\n",
    "    # 4. Extra cleanup by removing terms with a length of 1.\n",
    "    tweets = tweets.apply(lambda x: [words for words in x if words not in stop_words and len(words)>1 and words not in ['amp', 'rt'] and words not in string.punctuation])\n",
    "\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = clean_data(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [father, dysfunctional, selfish, drags, kids, ...\n",
       "1        [thanks, lyft, credit, can't, use, cause, offe...\n",
       "2                                        [bihday, majesty]\n",
       "3                           [model, love, take, time, urð]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "31957                                    [ate, isz, youuu]\n",
       "31958    [see, nina, turner, airwaves, trying, wrap, ma...\n",
       "31959    [listening, sad, songs, monday, morning, otw, ...\n",
       "31960    [sikh, temple, vandalised, calgary, wso, conde...\n",
       "31961                                      [thank, follow]\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check out the top terms in the tweets:\n",
    "##### 5.1. First, get all the tokenized terms into one large list.\n",
    "##### 5.2. Use the counter and find the 10 most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. First, get all the tokenized terms into one large list.\n",
    "tokens = [j for i in tweets for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Use the counter and find the 10 most common terms.\n",
    "\n",
    "counter = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 2667,\n",
       " 'day': 2249,\n",
       " 'happy': 1678,\n",
       " 'time': 1115,\n",
       " 'like': 1095,\n",
       " 'life': 1094,\n",
       " \"i'm\": 1011,\n",
       " 'today': 1002,\n",
       " 'new': 987,\n",
       " 'positive': 928}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = dict(sorted(counter.items(),key= lambda x:x[1], reverse=True)[:10])\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data formatting for predictive modeling:\n",
    "\n",
    "###### 1. Join the tokens back to form strings. This will be required for the vectorizers.\n",
    "###### 2. Assign x and y.\n",
    "###### 3. Perform train_test_split using sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Join the tokens back to form strings. This will be required for the vectorizers.\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit can't use cause offer wheel...\n",
       "2                                       bihday majesty\n",
       "3                             model love take time urð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Assign x and y.\n",
    "\n",
    "X = tweets\n",
    "y = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3. Perform train_test_split using sklearn.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373,)\n",
      "(22373,)\n",
      "(9589,)\n",
      "(9589,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. We’ll use TF-IDF values for the terms as a feature to get into a vector space model.\n",
    "###### 1. Import TF-IDF  vectorizer from sklearn.\n",
    "###### 2. Instantiate with a maximum of 5000 terms in your vocabulary.\n",
    "###### 3. Fit and apply on the train set.\n",
    "###### 4. Apply on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=5000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "tfidf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vect.get_feature_names()\n",
    "\n",
    "X_train_dtm = tfidf_vect.transform(X_train)\n",
    "X_train_dtm_df = pd.DataFrame(X_train_dtm.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dtm = tfidf_vect.transform(X_test)\n",
    "X_test_dtm_df = pd.DataFrame(X_test_dtm.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model building: Ordinary Logistic Regression\n",
    "\n",
    "###### 1. Instantiate Logistic Regression from sklearn with default parameters.\n",
    "###### 2. Fit into  the train data.\n",
    "###### 3. Make predictions for the train and the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reg = LogisticRegression()\n",
    "lr_reg.fit(X_train_dtm_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lr_reg.predict(X_test_dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = lr_reg.predict(X_train_dtm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Model evaluation: Accuracy, recall, and f_1 score.\n",
    "\n",
    "###### 1. Report the accuracy on the train set.\n",
    "###### 2. Report the recall on the train set: decent, high, or low.\n",
    "###### 3. Get the f1 score on the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9512983627072688\n",
      "Train Accuracy:  0.9551691771331515\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: ',accuracy_score(y_test, test_pred))\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Classification Report for Train ------- \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     20780\n",
      "           1       0.96      0.39      0.55      1593\n",
      "\n",
      "    accuracy                           0.96     22373\n",
      "   macro avg       0.96      0.69      0.76     22373\n",
      "weighted avg       0.96      0.96      0.95     22373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------ Classification Report for Train ------- \\n\\n')\n",
    "\n",
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Classification Report for Test ------- \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      8940\n",
      "           1       0.89      0.32      0.47       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.92      0.66      0.72      9589\n",
      "weighted avg       0.95      0.95      0.94      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------ Classification Report for Test ------- \\n\\n')\n",
    "\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy on the train set is : 95%\n",
    "###### Recall on the Train set is: 100% for Non-Toxic and 32% for Toxic, which is low\n",
    "###### f1 score for Train set is: 97% for Nob-Toxic and 47% for Toxic, which is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Looks like you need to adjust the class imbalance, as the model seems to focus on the 0s.\n",
    "###### 1. Adjust the appropriate class in the LogisticRegression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE()\n",
    "X_train_dtm_df, y_train = sm.fit_resample(X_train_dtm_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41560, 5000)\n",
      "(41560,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_dtm_df.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Train again with the adjustment and evaluate.\n",
    "\n",
    "###### 1. Train the model on the train set.\n",
    "###### 2. Evaluate the predictions on the train set: accuracy, recall, and f_1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11.1. Train the model on the train set.\n",
    "\n",
    "lr_reg_sm = LogisticRegression()\n",
    "lr_reg_sm.fit(X_train_dtm_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2. Evaluate the predictions on the train set: accuracy, recall, and f_1 score.\n",
    "\n",
    "train_pred = lr_reg_sm.predict(X_train_dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9564244465832531\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: ', accuracy_score(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Classification Report for Train ------- \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96     20780\n",
      "           1       0.93      0.98      0.96     20780\n",
      "\n",
      "    accuracy                           0.96     41560\n",
      "   macro avg       0.96      0.96      0.96     41560\n",
      "weighted avg       0.96      0.96      0.96     41560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------ Classification Report for Train ------- \\n\\n')\n",
    "\n",
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "###### Accuracy on the train set is : 96%\n",
    "###### Recall on the Train set is: 93% for Non-Toxic and 98% for Toxic, which is high\n",
    "###### f1 score for Train set is: 96% for Non-Toxic and 96% for Toxic, which is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Classification Report for Test ------- \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      8940\n",
      "           1       0.89      0.32      0.47       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.92      0.66      0.72      9589\n",
      "weighted avg       0.95      0.95      0.94      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------ Classification Report for Test ------- \\n\\n')\n",
    "\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8914   26]\n",
      " [ 441  208]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Regularization and Hyperparameter tuning:\n",
    "\n",
    "###### 1. Import GridSearch and StratifiedKFold because of class imbalance.\n",
    "\n",
    "###### 2. Provide the parameter grid to choose for ‘C’ and ‘penalty’ parameters.\n",
    "\n",
    "###### 3. Use a balanced class weight while instantiating the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import GridSearch and StratifiedKFold because of class imbalance.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Provide the parameter grid to choose for ‘C’ and ‘penalty’ parameters.\n",
    "\n",
    "grid = {\"C\": np.logspace(-3,3,7), \"penalty\": [\"l1\",\"l2\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Find the parameters with the best recall in cross validation.\n",
    "\n",
    "###### 1. Choose ‘recall’ as the metric for scoring.\n",
    "\n",
    "###### 2. Choose stratified 4 fold cross validation scheme.\n",
    "\n",
    "###### 3. Fit into  the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, estimator=LogisticRegression(),\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "gd_lr = GridSearchCV(lr_model, param_grid=grid, scoring=\"recall\", cv=4)\n",
    "gd_lr.fit(X_train_dtm_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. What are the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000.0, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_lr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Predict and evaluate using the best estimator.\n",
    "\n",
    "###### 1. Use the best estimator from the grid search to make predictions on the test set.\n",
    "\n",
    "###### 2. What is the recall on the test set for the toxic comments?\n",
    "\n",
    "###### 3. What is the f_1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Use the best estimator from the grid search to make predictions on the test set.\n",
    "\n",
    "lr_model = LogisticRegression(C=1000.0, penalty='l2')\n",
    "lr_model.fit(X_train_dtm_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = lr_model.predict(X_test_dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test: \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      8940\n",
      "           1       0.44      0.67      0.53       649\n",
      "\n",
      "    accuracy                           0.92      9589\n",
      "   macro avg       0.71      0.80      0.74      9589\n",
      "weighted avg       0.94      0.92      0.93      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. What is the recall on the test set for the toxic comments?\n",
    "\n",
    "print(\"Classification Report on Test: \\n\\n\")\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ans: Recall on the Toxic Comments is: 67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the f_1 score?\n",
    "\n",
    "##### Ans: f_1 score for Toxic comments is: 53% and for non-toxic comment is: 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
